초록

  우리는 LIME이라는 새로운 설명 기술을 제안합니다. 
  이 기술은 예측을 이해할 수 있고 신뢰할 수 있는 방식으로, 예측 주변에서 해석 가능한 모델을 학습함으로써 어떤 분류기의 예측도 설명합니다. 
  한, 대표적인 개별 예측과 그 설명을 중복되지 않는 방식으로 제시함으로써 모델을 설명하는 방법을 제안합니다. 이 작업은 부분적 최적화 문제로 구성됩니다.
  우리는 텍스트(예: 랜덤 포레스트)와 이미지 분류(예: 신경망)에 대한 다양한 모델을 설명함으로써 이 방법들의 유연성을 보여줍니다.
  우리는 신뢰가 필요한 다양한 시나리오에서 설명의 유용성을 새로운 실험을 통해 보여줍니다.
  이 실험에는 모의 실험과 인간 대상 실험이 포함되며, 예측을 신뢰해야 하는지 결정하기, 모델 선택하기, 신뢰할 수 없는 분류기 개선하기, 분류기를 신뢰할 수 없는 이유 식별하기   등이 있습니다.
    - 실험에 대한 내용이 무엇인지? 파악하기

소개

  [예측이나 모델에 대한 trust가 필요하다.]
  과학과 기술의 최근 발전의 핵심에 기계 학습이 있다는 것을 강조합니다.
  불행히도, 이 분야에서 인간의 중요한 역할은 종종 간과되는 측면입니다.
  사람들이 기계 학습 분류기를 도구로 직접 사용하든, 다른 제품 내에서 모델을 배포하든, 중요한 걱정거리는 남아있습니다:
  사용자가 모델이나 예측을 신뢰하지 않으면, 그것을 사용하지 않을 것입니다.
  예측을 신뢰하는 것과 모델을 신뢰하는 것, 두 가지 다른(하지만 관련된) 신뢰의 정의를 구분하는 것이 중요합니다:
  (1) 예측을 신뢰하는 것, 즉 사용자가 개별 예측을 충분히 신뢰하여 그것에 기반한 어떤 행동을 취할지 여부,
  그리고 (2) 모델을 신뢰하는 것, 즉 사용자가 배포됐을 때 모델이 합리적인 방식으로 행동할 것이라고 신뢰하는지 여부입니다.
  두 가지 모두 인간이 모델의 행동을 얼마나 이해하는지에 직접적으로 영향을 받으며, 이는 모델을 단순히 블랙 박스로 보는 것과 대비됩니다.

  [trust의 필요성]
  개별 예측에 대한 신뢰를 결정하는 것은 모델이 의사 결정에 사용될 때 중요한 문제입니다.
  예를 들어, 의료 진단이나 테러 탐지에 기계 학습을 사용할 때, 예측에 대한 맹목적인 신뢰로 행동할 수는 없습니다.
  왜냐하면 그 결과는 재앙적일 수 있기 때문입니다.
  ~
  개별 예측을 신뢰하는 것 외에도, 야생에서 배포하기 전에 모델 전체를 평가할 필요가 있습니다.
  이 결정을 내리기 위해, 사용자는 관심 있는 메트릭에 따라 모델이 실제 세계 데이터에서 잘 수행될 것이라고 확신할 필요가 있습니다.
  현재, 모델은 사용 가능한 검증 데이터 세트에서의 정확도 메트릭을 사용하여 평가됩니다.
  하지만, 실제 세계 데이터는 종종 상당히 다르며, 더욱이, 평가 메트릭이 제품의 목표를 나타내지 않을 수도 있습니다.
  이러한 메트릭에 더하여, 개별 예측과 그 설명을 검사하는 것은 가치 있는 해결책입니다.
  이 경우, 특히 대규모 데이터 세트의 경우, 검사할 인스턴스를 사용자에게 제안함으로써 사용자를 돕는 것이 중요합니다.
    - 인스턴스를 사용자에게 제안함으로써 돕는다..?

  이 논문에서 우리는 개별 예측에 대한 설명을 제공하는 것을 예측을 신뢰하는 문제에 대한 해결책으로 제안하며,
  여러 개의 예측(및 설명)을 선택하는 것을 모델을 신뢰하는 문제에 대한 해결책으로 제안합니다. 우리의 주요 기여는 다음과 같이 요약됩니다.
  LIME : 해석가능한 모델로 로컬리하게 근사 - 예측의 신뢰
  SP-LIME : submodular 최적화를 통한 모델의 신뢰
  실험 : 비전문가~, 뉴스그룹~, 딥러닝모델 불신을 왜 해야하는지?






