초록

우리는 LIME이라는 새로운 설명 기술을 제안합니다. 
이 기술은 예측을 이해할 수 있고 신뢰할 수 있는 방식으로, 예측 주변에서 해석 가능한 모델을 학습함으로써 어떤 분류기의 예측도 설명합니다. 
한, 대표적인 개별 예측과 그 설명을 중복되지 않는 방식으로 제시함으로써 모델을 설명하는 방법을 제안합니다. 이 작업은 부분적 최적화 문제로 구성됩니다.
우리는 텍스트(예: 랜덤 포레스트)와 이미지 분류(예: 신경망)에 대한 다양한 모델을 설명함으로써 이 방법들의 유연성을 보여줍니다.
우리는 신뢰가 필요한 다양한 시나리오에서 설명의 유용성을 새로운 실험을 통해 보여줍니다.
이 실험에는 모의 실험과 인간 대상 실험이 포함되며, 예측을 신뢰해야 하는지 결정하기, 모델 선택하기, 신뢰할 수 없는 분류기 개선하기, 분류기를 신뢰할 수 없는 이유 식별하기 등이 있습니다.
- 실험에 대한 내용이 무엇인지? 파악하기

소개

과학과 기술의 최근 발전의 핵심에 기계 학습이 있다는 것을 강조합니다. 불행히도, 이 분야에서 인간의 중요한 역할은 종종 간과되는 측면입니다. 사람들이 기계 학습 분류기를 도구로 직접 사용하든, 다른 제품 내에서 모델을 배포하든, 중요한 걱정거리는 남아있습니다: 사용자가 모델이나 예측을 신뢰하지 않으면, 그것을 사용하지 않을 것입니다. 예측을 신뢰하는 것과 모델을 신뢰하는 것, 두 가지 다른(하지만 관련된) 신뢰의 정의를 구분하는 것이 중요합니다: (1) 예측을 신뢰하는 것, 즉 사용자가 개별 예측을 충분히 신뢰하여 그것에 기반한 어떤 행동을 취할지 여부, 그리고 (2) 모델을 신뢰하는 것, 즉 사용자가 배포됐을 때 모델이 합리적인 방식으로 행동할 것이라고 신뢰하는지 여부입니다. 두 가지 모두 인간이 모델의 행동을 얼마나 이해하는지에 직접적으로 영향을 받으며, 이는 모델을 단순히 블랙 박스로 보는 것과 대비됩니다.
